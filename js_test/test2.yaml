protocolVersion: 2
name: tensorflow_cifar10
type: job
version: 1.0
contributor: Alice
description: image classification, cifar10 dataset, tensorflow, distributed training

prerequisites:
  - protocolVersion: 2
    name: tf_example
    type: dockerimage
    version: latest
    contributor: Alice
    description: python3.5, tensorflow
    auth:
      username: user
      password: <% $secrets.docker_password %>
      registryuri: openpai.azurecr.io
    uri: openpai/pai.example.tensorflow
  - protocolVersion: 2
    name: tensorflow_cifar10_model
    type: output
    version: latest
    contributor: Alice
    description: cifar10 data output
    uri: hdfs://10.151.40.179:9000/core/cifar10_model
  - protocolVersion: 2
    name: tensorflow_cnnbenchmarks
    type: script
    version: 84820935288cab696c9c2ac409cbd46a1f24723d
    contributor: MaggieQi
    description: tensorflow benchmarks
    uri: github.com/MaggieQi/benchmarks
  - protocolVersion: 2
    name: cifar10
    type: data
    version: latest
    contributor: Alice
    description: cifar10 dataset, image classification
    uri:
      - https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

parameters:
  model: resnet20
  batchsize: 32

secrets:
  docker_password: password
  github_token: cGFzc3dvcmQ=

jobRetryCount: 1
taskRoles:
  A:
    instances: 1
    resourcePerInstance:
      cpu: 2
      memoryMB: 16384
      gpu: 16
      ports:
        ssh: 1
        http: 1
    commands:
      - sleep infinity
  B:
    instances: 3
    resourcePerInstance:
      cpu: 2
      memoryMB: 16384
      gpu: 8
      ports:
        ssh: 1
        http: 1
    commands:
      - sleep infinity
  C:
    instances: 1
    resourcePerInstance:
      cpu: 2
      memoryMB: 16384
      gpu: 4
      ports:
        ssh: 1
        http: 1
    commands:
      - sleep infinity
  D:
    instances: 2
    resourcePerInstance:
      cpu: 2
      memoryMB: 16384
      gpu: 3
      ports:
        ssh: 1
        http: 1
    commands:
      - sleep infinity
  E:
    instances: 2
    resourcePerInstance:
      cpu: 2
      memoryMB: 16384
      gpu: 16
      ports:
        ssh: 1
        http: 1
    commands:
      - sleep infinity
  F:
    instances: 2
    resourcePerInstance:
      cpu: 2
      memoryMB: 16384
      gpu: 3
      ports:
        ssh: 1
        http: 1
    commands:
      - sleep infinity
  

deployments:
  - name: prod # This implementation will download the data to local disk, and the computed model will be output to local disk first and then being copied to hdfs.
    version: 1.0
    taskRoles:
      worker:
        preCommands:
          - wget <% $data.uri[0] %> -P data_<% $data.name %> # If local data cache deployed, one can copy data from local cache, only wget in case of cache miss.
          - >
            git clone https://<% $script.contributor %>:<% $secrets.github_token %>@<% $script.uri %> script_<% $script.name %> &&
            cd script_<% $script.name %> && git checkout <% $script.version %> && cd ..
            # Then the system will go ahead to execute worker's command.
      ps_server:
        preCommands:
          - wget <% $data.uri[0] %> -P data_<% $data.name %>
          - >
            git clone https://<% $script.contributor %>:<% $secrets.github_token %>@<% $script.uri %> script_<% $script.name %> &&
            cd script_<% $script.name %> && git checkout <% $script.version %> && cd ..
            # Then the system will go ahead to execute ps_server's command.
        postCommands:
          # After the execution of ps_server's command, the system goes here.
          - hdfs dfs -cp output_<% $output.name %> <% $output.uri %>
          # Assume the model is output locally, and this command copies the local output to hdfs. One can output to hdfs directly.
          # In this case, you will have to change "--train_dir=$PAI_WORK_DIR/output_<% $output.name %>".

defaults:
  deployment: prod # Use prod deployment in job submission.

extras:
  submitFrom: plugin
  hivedScheduler:
    jobPriorityClass: prod
    taskRoles:
      A:
        gpuType: DGX2-V100
        affinityGroupName: PCN-ABC
      B:
        gpuType: DGX2-V100
        affinityGroupName: PCN-ABC
      C:
        gpuType: DGX2-V100
        affinityGroupName: PCN-ABC
      D:
        gpuType: null
        affinityGroupName: PCN-D
      E:
        gpuType: DGX2-V100
        affinityGroupName: null
      F:
        reservationId: VC1-YQW-IB-DGX2
        affinityGroupName: PCN-F