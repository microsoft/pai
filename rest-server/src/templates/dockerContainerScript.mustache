#!/bin/bash

# Copyright (c) Microsoft Corporation
# All rights reserved.
#
# MIT License
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
# to permit persons to whom the Software is furnished to do so, subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


# Bootstrap script for docker container.

trap "kill 0" EXIT

touch "/alive/docker_$PAI_CONTAINER_ID"
while /bin/true; do
  [ $(( $(date +%s) - $(stat -c %Y /alive/yarn_$PAI_CONTAINER_ID) )) -gt 60 ] \
    && pkill -9 --ns 1
  sleep 20
done &


printf "%s %s\n%s\n\n" "[INFO]" "HADOOP CLASSPATH" "$(hadoop classpath --glob)"

export PAI_WORK_DIR="$(pwd)"
export PAI_DEFAULT_FS_URI={{{ hdfsUri }}}
HDFS_LAUNCHER_PREFIX=$PAI_DEFAULT_FS_URI/Container
export CLASSPATH="$(hadoop classpath --glob)"

export PAI_JOB_NAME={{{ jobData.jobName }}}
export PAI_USER_NAME={{{ jobData.username }}}
export PAI_DATA_DIR={{{ jobData.dataDir }}}
export PAI_OUTPUT_DIR={{{ jobData.outputDir }}}
export PAI_CODE_DIR={{{ jobData.codeDir }}}
export PAI_CURRENT_TASK_ROLE_NAME={{{ taskData.name }}}
export PAI_CURRENT_TASK_ROLE_TASK_COUNT={{{ taskData.taskNumber }}}
export PAI_CURRENT_TASK_ROLE_CPU_COUNT={{{ taskData.cpuNumber }}}
export PAI_CURRENT_TASK_ROLE_MEM_MB={{{ taskData.memoryMB }}}
export PAI_CURRENT_TASK_ROLE_GPU_COUNT={{{ taskData.gpuNumber }}}
export PAI_JOB_TASK_COUNT={{{ tasksNumber }}}
export PAI_JOB_TASK_ROLE_COUNT={{{ taskRolesNumber }}}
export PAI_JOB_TASK_ROLE_LIST={{{ taskRoleList }}}
export PAI_KILL_ALL_ON_COMPLETED_TASK_NUM={{{ jobData.killAllOnCompletedTaskNumber }}}


export PAI_CONTAINER_HOST_PORT=$(( (RANDOM % 55535) + 10001 ))
export PAI_CONTAINER_SSH_PORT=$(( (RANDOM % 55535) + 10001 ))
printf "%s %s\n" \
  "[INFO]" "PAI_CONTAINER_SSH_PORT is ${PAI_CONTAINER_SSH_PORT}"

task_role_no={{{ idx }}}
# touch a container id file in "APP_ID-TASK_ROLE_NO-TASK_INDEX-CONTAINER_HOST_IP-CONTAINER_HOST_PORT" format on hdfs
# to communicate with other containers, add APP_ID as prefix to differentiate attempts with same job name
hdfs dfs -touchz $HDFS_LAUNCHER_PREFIX/$PAI_USER_NAME/$PAI_JOB_NAME/tmp/$APP_ID-$task_role_no-$PAI_TASK_INDEX-$PAI_CONTAINER_HOST_IP-$PAI_CONTAINER_HOST_PORT || exit 1
while [ `hdfs dfs -ls $HDFS_LAUNCHER_PREFIX/$PAI_USER_NAME/$PAI_JOB_NAME/tmp/ | grep "/$PAI_JOB_NAME/tmp/$APP_ID-" | wc -l` -lt  $PAI_JOB_TASK_COUNT ]; do
  printf "%s %s\n" "[INFO]" "Waiting for other containers ..."
  sleep 10
done
hdfs dfs -ls $HDFS_LAUNCHER_PREFIX/$PAI_USER_NAME/$PAI_JOB_NAME/tmp/ \
  | grep "/$PAI_JOB_NAME/tmp/$APP_ID-" \
  | grep -oE "[^/]+$" \
  | sed -e "s/^$APP_ID-//g" \
  | sort -n -k 2 -t"-" \
  > ContainerList
if [ "$(cat ContainerList | wc -l)" -ne $PAI_JOB_TASK_COUNT ]; then
  printf "%s %s\n%s\n%s\n\n" \
    "[ERROR]" "ContainerList" \
    "$(cat ContainerList)" \
    "$(cat ContainerList | wc -l) containers are available, not equal to $PAI_JOB_TASK_COUNT, exit ..."
  exit 2
fi

function Prepare_SSH()
{
   printf "%s %s\n" "[INFO]" "in Prepare_SSH"
   mkdir /var/run/sshd
   mkdir /root/.ssh
   echo 'root:screencast' | chpasswd
   sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config
   sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd
   echo "export VISIBLE=now" >> /etc/profile
   export NOTVISIBLE="in users profile"
}
Prepare_SSH
printf "%s %s\n" "[INFO]" "NOTVISIBLE is ${NOTVISIBLE}"


function Start_SSH_Service()
{
  printf "%s %s\n" "[INFO]" "in Start_SSH"
  cat /root/.ssh/$APP_ID.pub >> /root/.ssh/authorized_keys
  sed -i 's/Port.*/Port '$PAI_CONTAINER_SSH_PORT'/' /etc/ssh/sshd_config
  echo "sshd:ALL" >> /etc/hosts.allow
  service ssh restart
}

function Hdfs_Upload_Atomically()
{
   printf "%s %s\n" "[INFO]" "in Hdfs_Upload_Atomically"
   printf "%s %s\n" "[INFO]" "destination path is ${2}"
   printf "%s %s\n" "[INFO]" "source path is ${1}"
   tempFolder=${2}"_temp"
   if [ Hdfs_Test_Path $tempFolder "directory" == 0 ]; then
    hdfs dfs -rm -r $tempFolder || exit 1
   fi
   hdfs dfs -copyFromLocal ${1} "$tempFolder" || exit 1
   hdfs dfs -mv "$tempFolder" ${2} || exit 1
}

function Hdfs_Test_Path()
{
   printf "%s %s\n" "[INFO]" "in Hdfs_Test_Path"
   printf "%s %s\n" "[INFO]" "test path is ${1}"
   printf "%s %s\n" "[INFO]" "test type is ${2}"
   if [ ${2} -eq "directory" ]; then
     hdfs dfs -test -f ${1} || exit 1
   else
     hdfs dfs -test -d ${1} || exit 1
   fi
}

# let taskRoleNumber=0 and taskindex=0 execute upload ssh files
hdfs_ssh_foler=${HDFS_LAUNCHER_PREFIX}/${PAI_USER_NAME}/${PAI_JOB_NAME}/ssh/${APP_ID}
printf "%s %s\n" \
  "[INFO]" "hdfsSSHFoler is ${hdfs_ssh_foler}"
printf "%s %s\n" \
  "[INFO]" "task_role_no is ${task_role_no}"
printf "%s %s\n" \
  "[INFO]" "PAI_TASK_INDEX is ${PAI_TASK_INDEX}"
if [ $task_role_no == 0 ] && [ $PAI_TASK_INDEX == 0 ]; then
  printf "%s %s %s\n%s\n" \
    "[INFO]" "task_role_no:${task_role_no}" "PAI_TASK_INDEX:${PAI_TASK_INDEX}"\
    "Execute upload key pair ..."
  ssh-keygen -N '' -t rsa -f ~/.ssh/$APP_ID
  hdfs dfs -mkdir "$hdfs_ssh_foler" || exit 1
  Hdfs_Upload_Atomically "/root/.ssh/" "$hdfs_ssh_foler/.ssh"
else
  # waiting for ssh key-pair ready
  while [ Hdfs_Test_Path "${hdfs_ssh_foler}/.ssh" "directory" != 0 ]; do
    sleep 10
  done
  printf "%s %s\n%s %s\n" \
      "[INFO]" "Ssh key pair ready ..." \
      "[INFO]" "Begin to download ssh key pair from hdfs ..."
  hdfs dfs -copyToLocal "${hdfs_ssh_foler}/.ssh/" "/root/.ssh/" || exit 1
fi
# Start ssh service
Start_SSH_Service
# Generate ssh connect info file in "APP_ID-PAI_CURRENT_CONTAINER_IP-PAI_CONTAINER_SSH_PORT" format on hdfs
hdfs dfs -touchz ${hdfsSSHFoler}/$APP_ID-$PAI_CURRENT_CONTAINER_IP-$PAI_CONTAINER_SSH_PORT || exit 1

export PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX=$((`cat ContainerList | grep "^$task_role_no-" | grep -n "$task_role_no-$PAI_TASK_INDEX-$PAI_CONTAINER_HOST_IP-$PAI_CONTAINER_HOST_PORT" | cut -d ":" -f 1`-1))
task_role_list=(${PAI_JOB_TASK_ROLE_LIST//,/ })
for i in `seq 0 $((PAI_JOB_TASK_ROLE_COUNT-1))`; do
  host_list=`cat ContainerList | grep "^$i-" | cut -d "-" -f 3-4 | tr "-" ":" | sed -e :E -e "N;s/\n/,/;tE"`
  export PAI_TASK_ROLE_${i}_HOST_LIST=$host_list
  export PAI_TASK_ROLE_${task_role_list[$i]}_HOST_LIST=$host_list
done
rm ContainerList

printf "%s %s\n%s\n\n" "[INFO]" "ENV" "$(printenv | sort)"

if [[ -n $PAI_CODE_DIR ]]; then
  hdfs dfs -get $PAI_CODE_DIR || exit 1
fi

# backward compatibility
export PAI_USERNAME=$PAI_USER_NAME
export PAI_TASK_ROLE_NAME=$PAI_CURRENT_TASK_ROLE_NAME
export PAI_TASK_ROLE_NUM=$PAI_CURRENT_TASK_ROLE_TASK_COUNT
export PAI_TASK_CPU_NUM=$PAI_CURRENT_TASK_ROLE_CPU_COUNT
export PAI_TASK_MEM_MB=$PAI_CURRENT_TASK_ROLE_MEM_MB
export PAI_TASK_GPU_NUM=$PAI_CURRENT_TASK_ROLE_GPU_COUNT
export PAI_TASK_ROLE_INDEX=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX
export PAI_TASKS_NUM=$PAI_JOB_TASK_COUNT
export PAI_TASK_ROLES_NUM=$PAI_JOB_TASK_ROLE_COUNT
export PAI_TASK_ROLE_LIST=$PAI_JOB_TASK_ROLE_LIST
export PAI_CURRENT_CONTAINER_IP=$PAI_CONTAINER_HOST_IP
export PAI_CURRENT_CONTAINER_PORT=$PAI_CONTAINER_HOST_PORT

printf "%s %s\n\n" "[INFO]" "USER COMMAND START"
{{{ taskData.command }}} || exit $?
printf "\n%s %s\n\n" "[INFO]" "USER COMMAND END"

{{# jobData.killAllOnCompletedTaskNumber }}
hdfs dfs -touchz $HDFS_LAUNCHER_PREFIX/$PAI_USER_NAME/$PAI_JOB_NAME/finished/$APP_ID-$PAI_TASK_INDEX || exit 1
while [ `hdfs dfs -ls $HDFS_LAUNCHER_PREFIX/$PAI_USER_NAME/$PAI_JOB_NAME/finished/ | grep "/$PAI_JOB_NAME/finished/$APP_ID-" | wc -l` -lt  $PAI_KILL_ALL_ON_COMPLETED_TASK_NUM ]; do
  sleep 10
done
{{/ jobData.killAllOnCompletedTaskNumber }}

exit 0
