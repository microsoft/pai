<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
  <name>dfs.replication</name>
  <value>3</value>
  <description>
         Default block replication. The actual number of replicas can be specified when
         the file is created. The default is used if replication is not specified at
         create time.
  </description>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>file:///var/lib/hdfs/name</value>
  <description>
         This property specifies the URIs of the directories where the NameNode
         stores its metadata and edit logs.
  </description>
</property>


<property>
  <name>dfs.namenode.handler.count</name>
  <value>100</value>
  <description>
         The number of server threads for the namenode. (default is 10)
  </description>
</property>

<property>
  <name>dfs.datanode.handler.count</name>
  <value>50</value>
  <description>
         The number of server threads for the datanode. (default is 10)
  </description>
</property>

<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>8192</value>
  <description>
         Specifies the maximum number of threads to use for transferring data in and
         out of the DN. (default is 4096)
  </description>
</property>

<property>
  <name>dfs.datanode.balance.bandwidthPerSec</name>
  <value>536870912</value>
  <description>
         Specifies the maximum amount of bandwidth that each datanode can utilize for
         the balancing purpose in term of the number of bytes per second.
         (default is 1048576, but since we have 40Gb NIC we pick 10% of total bandwidth or 0.5 GB).
  </description>
</property>

<property>
  <name>dfs.datanode.balance.max.concurrent.moves</name>
  <value>100</value>
  <description>
         Specifies the maximum number of concurrent moves a balancer is allowed to make.
  </description>
</property>

<property>
  <name>dfs.datanode.du.reserved</name>
  <value>10737418240</value> <!-- 10 GB -->
  <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
  </description>
</property>

<!--
  Note: The description for dfs.datanode.fsdataset.volume.choosing.policy comes
  from https://issues.apache.org/jira/browse/HDFS-8538. It's not yet in hdfs-default.xml
  It lists AvailableSpaceVolumeChoosingPolicy as default, which it isn't (yet).
-->

<property>
  <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
  <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
  <description>
    The volume choosing policy dictates how the datanode decides to place a
    block among its volumes. Two policies are available.
    The default org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy
    chooses volumes based on their free space, attempting to fill disks with more
    available space first. The alternative
    org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy
    chooses volumes in a round-robin order, meaning that disks will less free
    space will fill up first.
  </description>
</property>

<property>
  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name>
  <value>42949672960</value> <!-- 40 GB -->
  <description>
    Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to
    org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.
    This setting controls how much DN volumes are allowed to differ in terms of
    bytes of free disk space before they are considered imbalanced. If the free
    space of all the volumes are within this range of each other, the volumes
    will be considered balanced and block assignments will be done on a pure
    round robin basis.
  </description>
</property>

<property>
  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name>
  <value>0.9</value>
  <description>
    Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to
    org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.
    This setting controls what percentage of new block allocations will be sent
    to volumes with more available disk space than others. This setting should
    be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should
    be no reason to prefer that volumes with less available disk space receive
    more block allocations.
  </description>
</property>

<property>
  <name>dfs.namenode.num.checkpoints.retained</name>
  <value>10</value>
  <description>
         The number of image checkpoint files (fsimage_*) that will be retained by the
         NameNode and Secondary NameNode in their storage directories. All edit logs
         (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained
         checkpoint will also be retained. (default is 2)
  </description>
</property>

<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
  <description>
         Enable REST access to HDFS without additional security in kerberos.
  </description>
</property>

<property>
  <name>dfs.namenode.rpc-bind-host</name>
  <value>0.0.0.0</value>
  <description>
         The actual address the RPC server will bind to. If this optional address is
         set, it overrides only the hostname portion of dfs.namenode.rpc-address.
         It can also be specified per name node or name service for HA/Federation.
         This is useful for making the name node listen on all interfaces by
         setting it to 0.0.0.0.
  </description>
</property>

<property>
  <name>dfs.namenode.servicerpc-bind-host</name>
  <value>0.0.0.0</value>
  <description>
         The actual address the service RPC server will bind to. If this optional address is
         set, it overrides only the hostname portion of dfs.namenode.servicerpc-address.
         It can also be specified per name node or name service for HA/Federation.
         This is useful for making the name node listen on all interfaces by
         setting it to 0.0.0.0.
  </description>
</property>

<property>
  <name>dfs.namenode.http-bind-host</name>
  <value>0.0.0.0</value>
  <description>
         The actual adress the HTTP server will bind to. If this optional address
         is set, it overrides only the hostname portion of dfs.namenode.http-address.
         It can also be specified per name node or name service for HA/Federation.
         This is useful for making the name node HTTP server listen on all
         interfaces by setting it to 0.0.0.0.
  </description>
</property>

<property>
  <name>dfs.namenode.https-bind-host</name>
  <value>0.0.0.0</value>
  <description>
         The actual adress the HTTPS server will bind to. If this optional address
         is set, it overrides only the hostname portion of dfs.namenode.https-address.
         It can also be specified per name node or name service for HA/Federation.
         This is useful for making the name node HTTPS server listen on all
         interfaces by setting it to 0.0.0.0.
  </description>
</property>

<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>false</value>
  <description>
         This configuration parameter turns on short-circuit local reads. It requires
         libhadoop.so native library to be installed with Hadoop.
  </description>
</property>

<property>
  <name>dfs.domain.socket.path</name>
  <value>/var/lib/hdfs/datanode_socket</value>
  <description>
         Optional. This is a path to a UNIX domain socket that will be used for
         communication between the DataNode and local HDFS clients. If the string
         "_PORT" is present in this path, it will be replaced by the TCP port of the
         DataNode.
  </description>
</property>

<property>
  <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
  <value>false</value>
  <description>
          If true (the default), then the namenode requires that a connecting datanode's
          address must be resolved to a hostname. If necessary, a reverse DNS lookup is
          performed. All attempts to register a datanode from an unresolvable address are
          rejected. It is recommended that this setting be left on to prevent accidental
          registration of datanodes listed by hostname in the excludes file during a DNS
          outage. Only set this to false in environments where there is no infrastructure
          to support reverse DNS lookup.
  </description>
</property>

<property>
  <name>dfs.namenode.accesstime.precision</name>
  <value>3600000</value>
  <description>
        The access time for HDFS file is precise upto this value.
        The default value is 1 hour. Setting a value of 0 disables access times for HDFS.
  </description>
</property>

<property>
  <name>dfs.namenode.acls.enabled</name>
  <value>true</value>
  <description>
        Set to true to enable support for HDFS ACLs (Access Control Lists). By default, ACLs are disabled.
        When ACLs are disabled, the NameNode rejects all RPCs related to setting or getting ACLs.
  </description>
</property>

<property>
    <name>dfs.namenode.http-address</name>
    <value>0.0.0.0:5070</value>
    <description>
        The address and the base port where the dfs namenode web ui will listen on.
    </description>
</property>

<property>
    <name>dfs.namenode.https-address</name>
    <value>0.0.0.0:5470</value>
    <description>
        The namenode secure http server address and port.
    </description>
</property>

<property>
    <name>dfs.namenode.backup.address</name>
    <value>0.0.0.0:5100</value>
    <description>
        The backup node server address and port. If the port is 0 then the server will start on a free port.
    </description>
</property>

<property>
    <name>dfs.namenode.backup.http-address</name>
    <value>0.0.0.0:5105</value>
    <description>
        The backup node http server address and port. If the port is 0 then the server will start on a free port.
    </description>
</property>

<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>0.0.0.0:5090</value>
    <description>
        The secondary namenode http server address and port.
    </description>
</property>

<property>
    <name>dfs.namenode.secondary.https-address</name>
    <value>0.0.0.0:5091</value>
    <description>
        The secondary namenode HTTPS server address and port.
    </description>
</property>

<property>
  <name>dfs.namenode.safemode.threshold-pct</name>
  <value>0.99f</value>
  <description>
        By default safemode threshold is 0.999, reduce to 0.99.
  </description>
</property>

<property>
  <name>dfs.namenode.safemode.min.datanodes</name>
  <value>1</value>
  <description>
        At least 1 datanode needed to leave safemode, default value is 0.
  </description>
</property>

<property>
  <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
  <value>true</value>
  <description>
      If there is a datanode/network failure in the write pipeline, DFSClient will
      try to remove the failed datanode from the pipeline and then continue writing
      with the remaining datanodes. As a result, the number of datanodes in the
      pipeline is decreased. The feature is to add new datanodes to the pipeline.
      This is a site-wide property to enable/disable the feature. When the cluster
      size is extremely small, e.g. 3 nodes or less, cluster administrators may want
      to set the policy to NEVER in the default configuration file or disable this feature.
      Otherwise, users may experience an unusually high rate of pipeline failures since it is
      impossible to find new datanodes for replacement.
  </description>
</property>

<property>
  <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
  <value>ALWAYS</value>
  <description>
      This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable
      is true. ALWAYS: always add a new datanode when an existing datanode is removed.
      NEVER: never add a new datanode. DEFAULT: Let r be the replication number.
      Let n be the number of existing datanodes. Add a new datanode only if r is greater than or equal to 3 and either
      (1) floor(r/2) is greater than or equal to n; or (2) r is greater than n and the block is hflushed/appended.
  </description>
</property>

<property>
  <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name>
  <value>true</value>
  <description>
      This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true.
      Best effort means that the client will try to replace a failed datanode in write pipeline (provided that the
      policy is satisfied), however, it continues the write operation in case that the datanode replacement also fails.
      Suppose the datanode replacement fails. false: An exception should be thrown so that the write will fail.
      true : The write should be resumed with the remaining datandoes. Note that setting this property to true allows
      writing to a pipeline with a smaller number of datanodes. As a result, it increases the probability of data loss.
  </description>
</property>

</configuration>
