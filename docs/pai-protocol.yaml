protocolVersion: String, required # Protocol version, current version is 2
name: String, required
type: String, required # The type of the component. Must be one of the following: job, data, script, dockerimage, or output
version: String, optional # Component version. Default is latest
contributor: String, optional
description: String, optional

prerequisites: # Optional
  - protocolVersion: String, optional # If omitted, follow the protocolVersion in root
    name: String, required
    type: String, required # Component type. Must be one of the following: data, script, dockerimage, or output. Prerequisites.type cannot be "job"
    version: String, optional # Component version. Default is latest
    contributor: String, optional
    description: String, optional
    auth: Object, optional # Only available when the type is dockerimage
      username: String, optional
      passward: String, optional
      registryuri: String, optional
    uri: String or list, required # Only when the type is data can the uri be a list

parameters: # Optional, can be omitted. If specified, the whole object can be referenced as `$parameters`
  <param1>: value1Type # Specify name and value of all the referencable parameters that will be used in the whole job template.
  <param2>: value2Type # Can be referenced by `<% $parameters.param1 %>`, `<% $parameters.param2 %>`

jobRetryCount: Integer, optional # Default is 0
taskRoles:
  <name>: String, required  # Name of the taskRole
   protocol_version: String, optional # Protocol version, default is 2
    instances: Integer, optional # Default is 1, instances of a taskRole
    completion:
      minFailedInstances: Integer or null, optional # Default 1
      minSucceededInstances: Integer or null, optional # Default null
    taskRetryCount: Integer, optional # Default is 0
    dockerImage: String, required # Should reference to a dockerimage defined in prerequisites
    data: String, optional # Should reference to data defined in prerequisites, the whole data object in this taskRole can be referenced as `$data`
    output: String, optional # Should reference to output defined in prerequisites, the whole output object in this taskRole can be referenced as `$output`
    script: String, optional # Should reference to script defined in prerequisites, the whole script object in this taskRole can be referenced as `$script`
    extraContainerOptions:
      shmMB: Integer, optional # config the /dev/shm in a docker container, https://docs.docker.com/compose/compose-file/#shm_size
    resourcePerInstance:
      cpu: Integer, required
      memoryMB: Integer, required
      gpu: Integer, required
      ports:
        <portLabel1>: Integer, optional, default is 0 # Only for host network
    commands:
      - String, required

# to handle that a component may interact with different component differently, user is encouraged to place the codes handling such difference in the "deployments" field.
# e.g., a job may get input data through wget, hdfc -dfs cp, copy, or just directly read from remote storage. This logic can be placed here.
# in summary, the deployments field is responsible to make sure the job to run properly in a deployment specific runtime environment.
# one could have many deployments, but only the first deployment can be activated at runtime. User can choose the deployment at job submission time.
deployments:
  - protocolVersion: String, optional # If omitted, follow the root protocolVersion
    name: String, required
    taskRoles:
      <name>: String, required # Should be in taskRoles
        preCommands:
          - String, required # execute before the taskRole's command
        postCommands:
          - String, required # execute after the taskRole's command

attachments: # optional, cluster specific parameters
  - protocolVersion: String, optional
    virtualCluster: String, optional

default: # optional, default settings used in the job
  deployment: String, optional # Should reference to deployment defined in deployments



# Below is an example for distributed tensorflow:

protocolVersion: 2
name: tensorflow_cifar10
type: job
version: 1.0.0
contributor: Alice
description: image classification, cifar10 dataset, tensorflow, distributed training

prerequisites:
  - protocolVersion: 2
    name: tf_example
    type: dockerimage
    version: latest
    contributor: Alice
    description: python3.5, tensorflow
    auth:
      username: 81f1fd6a-2844-4072-982d-62371fa37bd3
      passward: user1
      registryuri: openpai.azurecr.io
    uri: openpai/pai.example.tensorflow
  - protocolVersion: 2
    name: tensorflow_cifar10_model
    type: output
    version: latest
    contributor: Alice
    description: cifar10 data output
    uri: hdfs://10.151.40.179:9000/core/cifar10_model
  - protocolVersion: 2
    name: tensorflow_cnnbenchmarks
    type: script
    version: 84820935288cab696c9c2ac409cbd46a1f24723d
    contributor: MaggieQi
    description: tensorflow benchmarks
    uri: https://github.com/MaggieQi/benchmarks
  - protocolVersion: 2
    name: cifar10
    type: data
    version: latest
    contributor: Alice
    description: cifar10 dataset, image classification
    uri:
      - https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

parameters:
  model: resnet20
  batchsize: 32

jobRetryCount: 1
taskRoles:
  worker:
    protocolVersion: 2
    instance: 1
    completion:
      minFailedInstances: 1
      minSucceededInstances: 1
    taskRetryCount: 0
    dockerImage: tf_example
    data: cifar10
    output: tensorflow_cifar10_model
    script: tensorflow_cnnbenchmarks
    extraContainerOptions:
      shmMB: 64
    resourcePerInstance:
      cpu: 2
      memoryMB: 16384
      gpu: 4
      ports:
        ssh: 0
        http: 0
    commands:
      - cd script_<% $script.name %>/scripts/tf_cnn_benchmarks
      - >
        python tf_cnn_benchmarks.py --job_name=worker
        --local_parameter_device=gpu
        --variable_update=parameter_server
        --ps_hosts=$PAI_TASK_ROLE_ps_server_HOST_LIST
        --worker_hosts=$PAI_TASK_ROLE_worker_HOST_LIST
        --task_index=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX
        --data_name=<% $data.name %>
        --data_dir=$PAI_WORK_DIR/data_<% $data.name %>
        --train_dir=$PAI_WORK_DIR/output_<% $output.name %>
        --model=<% $parameters.model %>
        --batch_size=<% $parameters.batchsize %>
  ps_server:
    protocolVersion: 2
    instances: 1
    completion:
      minFailedInstances: 1
      minSucceededInstances: null
    taskRetryCount: 0
    dockerImage: tf_example
    data: cifar10
    output: tensorflow_cifar10_model
    script: tensorflow_cnnbenchmarks
    extraContainerOptions:
      shmMB: 64
    resourcePerInstanec:
      cpu: 2
      memoryMB: 8192
      gpu: 0
      ports:
        ssh: 0
        http: 0
    commands:
      - cd script_<% $script.name %>/scripts/tf_cnn_benchmarks
      - >
        python tf_cnn_benchmarks.py --job_name=ps
        --local_parameter_device=gpu
        --variable_update=parameter_server
        --ps_hosts=$PAI_TASK_ROLE_ps_server_HOST_LIST
        --worker_hosts=$PAI_TASK_ROLE_worker_HOST_LIST
        --task_index=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX
        --data_dir=$PAI_WORK_DIR/data_<% $data.name %>
        --data_name=<% $data.name %>
        --train_dir=$PAI_WORK_DIR/output_<% $output.name %>
        --model=<% $parameters.model %>
        --batch_size=<% $parameters.batchsize %>

deployments:
  - protocolVersion: 2
    name: prod # This implementation will download the data to local disk, and the computed model will be output to local disk first and then being copied to hdfs
    version: 1.0.0
    taskRoles:
      worker:
        preCommands:
          - wget <% $data.uri[0] %> -P data_<% $data.name %> # if local data cache deployed, one can copy data from local cache, only wget in case of cache miss
          - >
            git clone <% $script.uri %> script_<% $script.name %> &&
            cd script_<% $script.name %> && git checkout <% $script.version %> && cd ..
            # and the system will go ahead to execute worker's command
      ps_server:
        preCommands:
          - wget <% $data.uri[0] %> -P data_<% $data.name %>
          - >
            git clone <% $script.uri %> script_<% $script.name %> &&
            cd script_<% $script.name %> && git checkout <% $script.version %> && cd ..
            # and the system will go ahead to execute ps_server's command
        postCommands:
          # after the execution of ps_server's command, the system goes here
          - hdfs dfs -cp output_<% $output.name %> <% $output.uri %>
          # assume the model is output locally, and this command copies the local output to hdfs. One can output to hdfs directly.
          # In this case, you will have to change "--train_dir=$PAI_WORK_DIR/output_<% $output.name %>"

default:
  deployment: prod # use prod deployment
